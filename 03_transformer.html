<!DOCTYPE html>
<html>
  <head>
    <title>§3 Transformer</title>
    <meta charset="utf-8">

    <!-- KaTeX -->
    <link rel="stylesheet" href="./layouts/katex/katex.min.css">
    <script defer src="./layouts/katex/katex.min.js"></script>
    <script defer src="./layouts/katex/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>

    <!-- tabsets -->
    <link rel="stylesheet" href="./layouts/tabsets/tabsets.min.css">
    <script defer src="./layouts/tabsets/tabsets.min.js"></script>

    <link rel="stylesheet" href="./layouts/style.css" />
  </head>
  <body>
    <textarea id="source">

class: center, middle

<!-- copy and paste down below -->

## §3 Transformer

Transformer is a general function fitter with residual connections, which is a more general FFN.

---

### §3.1 Embedding

Embedding is ordered higher-dimensional representation vectors.

---

#### §3.1.1 `nn.Embedding`

[tiktoken](https://github.com/openai/tiktoken)

<div class="tabset"></div>

- `nn.Embedding`
    
    [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)
    
    ```python
    NUM_INDEX = 3
    EMBEDDING_DIM = 4
    
    embedding = nn.Embedding(NUM_INDEX, EMBEDDING_DIM)
    print(embedding.weight.detach())
    
    index = torch.LongTensor([2, 0])
    print(embedding(index))
    ```
    
    will get:
    
    ```Bash
    tensor([[ 0.0378,  1.0396, -0.9673,  0.9697],
            [-0.7824,  1.8141,  0.5336, -1.6396],
            [ 0.1903,  0.6592,  1.4589, -0.6018]])
    tensor([[ 0.1903,  0.6592,  1.4589, -0.6018],
            [ 0.0378,  1.0396, -0.9673,  0.9697]], grad_fn=<EmbeddingBackward0>)
    ```

- `F.one_hot`

    [F.one_hot](https://pytorch.org/docs/stable/generated/torch.nn.functional.one_hot.html)
    
    ```python
    one_hot = F.one_hot(index, num_classes=NUM_INDEX)
    print(one_hot)
    
    linear = nn.Linear(NUM_INDEX, EMBEDDING_DIM, bias=False)
    linear.weight = nn.Parameter(embedding.weight.T.detach())
    print(linear(one_hot.float()))
    ```
    
    will get:
    
    ```Bash
    tensor([[0, 0, 1],
            [1, 0, 0]])
    tensor([[ 0.1903,  0.6592,  1.4589, -0.6018],
            [ 0.0378,  1.0396, -0.9673,  0.9697]], grad_fn=<MmBackward0>)
    ```

---

#### §3.1.2 Sinusoidal Positional Embedding

<div class="tabset"></div>

- `class Embedding`
    
    ```python
    class Embedding(nn.Module):
        def __init__(self, hidden_dim=768, max_seq_length=5000):
            super().__init__()
            self.embedding = nn.Embedding(max_seq_length, hidden_dim)
            self.hidden_dim = hidden_dim
        def forward(self, x):
            return self.embedding(x) * math.sqrt(self.hidden_dim)
    ```

- `class PositionalEncoding`
    
    $PE_{(pos, 2j)} = \sin(\frac{pos}{10000^{2j/{dmodel}}})$
    
    $PE_{(pos, 2j + 1)} = \cos(\frac{pos}{10000^{2j/{dmodel}}})$
    
    where $pos$ is each element in the sequence up to `max_seq_length`, and $d_{model}$ is `hidden_dim`.
    
    ```python
    class PositionalEncoding(nn.Module):
        def __init__(self, hidden_dim=768, max_seq_length=5000, dropout=0.0):
            super().__init__()
            self.dropout = nn.Dropout(dropout)
            pe = torch.zeros(max_seq_length, hidden_dim)
            position = torch.arange(0, max_seq_length).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, hidden_dim, 2) * -(math.log(10000.0) / hidden_dim))
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            pe = pe.unsqueeze(0)
            self.pe = pe
        def forward(self, x):
            seq_length = x.shape[1]
            x = x + self.pe[:, :seq_length].requires_grad_(False)
            return self.dropout(x)
    ```

- testing
    
    ```python
    dummy = torch.randint(5000, (1, 196))# [batch_size, seq_length], words as int numbers
    embeddings = Embedding()
    dummy = embeddings(dummy)
    print(dummy.shape)# [batch_size, seq_length, hidden_dim]
    positional_encoding = PositionalEncoding()
    dummy = positional_encoding(dummy)
    print(dummy.shape)# [batch_size, seq_length, hidden_dim]
    ```
    
    will get:
    
    ```Bash
    torch.Size([1, 196, 768])
    torch.Size([1, 196, 768])
    ```

---

<div class="tabset"></div>

- `class SinusoidalPosEmb`
    
    ```python
    class SinusoidalPosEmb(nn.Module):
        def __init__(self, hidden_dim=768, M=10000):
            super().__init__()
            self.hidden_dim = hidden_dim
            self.M = M
    
        def forward(self, x):
            device = x.device
            half_dim = self.hidden_dim // 2
            emb = math.log(self.M) / half_dim
            emb = torch.exp(torch.arange(half_dim, device=device) * (-emb))
            emb = x[..., None] * emb[None, ...]
            emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
            return emb
    ```

- testing

    ```python
    dummy = torch.rand(1, 196)# [batch_size, seq_length], words as float numbers
    sinusoidal_pos_emb = SinusoidalPosEmb()
    dummy = sinusoidal_pos_emb(dummy)
    print(dummy.shape)# [batch_size, seq_length, hidden_dim]
    ```
    
    will get:
    
    ```Bash
    torch.Size([1, 196, 768])
    ```

---

### §3.2 Transformer Encoder

#### §3.2.1 FFN (MLP)

<div class="tabset"></div>

- Equation
    
    Feed Forward Network $$\text{FFN}(X)=(\text{ReLU}(0,XW_1+b_1))W_2+b_2$$, where $\text{ReLU}(x)=\max{(0,x)}$. Here we replace [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) with [nn.GELU](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html).

- `class FFN`

    ```python
    class FFN(nn.Module):
        def __init__(self, in_features=768, hidden_features=3072, out_features=768, dropout=0.0):
            super().__init__()
            self.linear1 = nn.Linear(in_features, hidden_features)
            self.act = nn.GELU()
            self.linear2 = nn.Linear(hidden_features, out_features)
            self.dropout = nn.Dropout(dropout)
    
        def forward(self, x):
            x = self.linear1(x)
            x = self.act(x)
            x = self.dropout(x)
            x = self.linear2(x)
            x = self.dropout(x)
            return x
    ```

---

#### §3.2.2 MultiheadAttention

<div class="tabset"></div>

- Equation
    
    [nn.MultiheadAttention](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html)
    
    We define $$\text{Attention}(Q, K, V) = \text{Softmax}(\frac{QK^\top}{\sqrt{d_{k}}})V$$, and $$\text{MultiheadAttention} (Q, K, V) = \text{Concat}(\text{head}_1, \cdots, \text{head}_h) W^0$$, where $\text{head}_i = \text{Attention} (QW^Q_i, KW^K_i, VW^V_i)$, and $h$ is `num_heads` in the code below.

- `class MultiheadAttention`
    
    ```python
    class MultiheadAttention(nn.Module):
        def __init__(self, hidden_dim=768, num_heads=12, dropout=0.0):
            super().__init__()
            self.num_heads = num_heads
            d_k = hidden_dim // num_heads
            self.scale = d_k ** -0.5
            self.qkv = nn.Linear(hidden_dim, hidden_dim * 3)
            self.dropout = nn.Dropout(dropout)
            self.proj = nn.Linear(hidden_dim, hidden_dim)
        def forward(self, x):
            batch_size, seq_length, hidden_dim = x.shape
            qkv = self.qkv(x)
            qkv = qkv.reshape(batch_size, seq_length, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)
            q, k, v = qkv[0], qkv[1], qkv[2]# q, k, v shape: [batch_size, num_heads, seq_length, hidden_dim // num_heads]
            attn = self.scale * q.float() @ (k.float().transpose(-2, -1))# `torch.matmul`
            attn = attn.softmax(dim=-1)
            attn = self.dropout(attn)
            x = (attn @ v).transpose(1, 2).reshape(batch_size, seq_length, hidden_dim)
            x = self.proj(x)
            x = self.dropout(x)
            return x
    ```

---

#### §3.2.3 TransformerEncoderLayer

[nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html)

```python
class TransformerEncoderLayer(nn.Module):
    def __init__(self, num_heads=12, hidden_dim=768, ffn_dim=3072, dropout=0.0):
        super().__init__()
        self.attention = MultiheadAttention(hidden_dim, num_heads, dropout=dropout)
        self.layer_norm_1 = nn.LayerNorm(hidden_dim)
        self.ffn = FFN(hidden_dim, ffn_dim, hidden_dim, dropout)
        self.layer_norm_2 = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(0.0)

    def forward(self, x):
        residual = x
        x = self.attention(x)
        x = self.layer_norm_1(x)
        x = self.dropout(x)
        x += residual

        residual = x
        x = self.ffn(x)
        x = self.layer_norm_2(x)
        x = self.dropout(x)
        x += residual
        return x
```

---

#### §3.2.4 TransformerEncoder

[nn.TransformerEncoder](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)

```python
class TransformerEncoder(nn.Module):
    def __init__(self, num_layers=12, num_heads=12, hidden_dim=768, ffn_dim=3072, dropout=0.0):
        super().__init__()
        self.transformer_encoder_layers = nn.ModuleList([
            TransformerEncoderLayer(num_heads, hidden_dim, ffn_dim, dropout)
            for _ in range(num_layers)
        ])

    def forward(self, x):
        for transformer_encoder_layer in self.transformer_encoder_layers:
            x = transformer_encoder_layer(x)
        return x
```

---

### §3.3 Encoder-Decoder, Encoder-Only, Decoder-Only

|Encoder-Decoder: seq2seq|Encoder-Decoder: Transformer|
|-|-|
|![](https://d2l.ai/_images/seq2seq-state.svg)|![](https://d2l.ai/_images/transformer.svg)|

---

| [_Understanding Transformer model architectures_ (practicalai.io)](https://www.practicalai.io/understanding-transformer-model-architectures/) | [_11.9. Large-Scale Pretraining with Transformers_ (d2l.ai)](https://d2l.ai/chapter_attention-mechanisms-and-transformers/large-pretraining-transformers.html) |

| |NLP|CV|
|:-|:-|:-|
|Encoder-Decoder|[[1706.03762] _Attention is All You Need_](https://arxiv.org/abs/1706.03762), [T5](https://arxiv.org/abs/1910.10683)|[BEiT](https://arxiv.org/abs/2106.08254)|
|Encoder-Only|[BERT](https://arxiv.org/abs/1810.04805)|[ViT](https://arxiv.org/abs/2010.11929)|
|Decoder-Only|[GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [GPT-3](https://arxiv.org/abs/2005.14165), [nanoGPT](https://github.com/karpathy/nanoGPT)||

---

### §3.4 Attention is All You Need

[[1706.03762] _Attention Is All You Need_](https://arxiv.org/abs/1706.03762)

- A pure Transformer structure instead of RNNs.
- Use [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) to let query $Q$ choose different $K^\mathsf{T}$.
- The encoder provides keys $K$ and value $V$, while the decoder provides query $Q$.

#### §3.4.1 fine-tuning of LLM

The ULMFiT 3-step approach (see Fig.1 of [[1801.06146] _Universal Language Model Fine-tuning for Text Classification_](https://arxiv.org/abs/1801.06146):
1. Language Model pre-training.
2. Instruction tuning.
3. RLHF (Reinforcement Learning from Human Feedback).

---

### §3.5 Vision Transformer (ViT)

[[2010.11929] _An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale_](https://arxiv.org/abs/2010.11929)

- A pure Transformer structure instead of CNNs.
- General function fitter instead of good inductive prior.
- With enough data.

---

<center><img src="https://d2l.ai/_images/vit.svg" width="800"></center>

---

#### §3.5.1 PatchEmbedding

<div class="tabset"></div>

- `class PatchEmbedding`
    
    ```python
    class PatchEmbedding(nn.Module):
        def __init__(self, in_channels=3, patch_size=16, hidden_dim=768):
            super().__init__()
            self.conv2d = nn.Conv2d(
                in_channels=in_channels,
                out_channels=hidden_dim,
                kernel_size=patch_size,
                stride=patch_size,
                padding=0
            )
            self.hidden_dim = hidden_dim
    
        def forward(self, x):
            batch_size = x.shape[0]
            x = self.conv2d(x)
            x = x.view(batch_size, -1, self.hidden_dim)
            return x
    ```

- `class PatchEmbedding`

    ```python
    class PatchEmbedding(nn.Module):
        def __init__(self, hidden_dim=768):
            super().__init__()
            self.hidden_dim = hidden_dim
    
        def forward(self, x):
            batch_size = x.shape[0]
            x = x.view(batch_size, -1, self.hidden_dim)
            return x
    ```

---

#### §3.5.2 VisionTransformer

```python
class VisionTransformer(nn.Module):
    def __init__(
        self, image_size=224, in_channels=3, patch_size=16, num_classes=1000,
        num_layers=12, num_heads=12, hidden_dim=768, ffn_dim=3072, dropout=0.0
        ):
        super().__init__()
        self.patch_embedding = PatchEmbedding(in_channels, patch_size, hidden_dim)
        self.pos_embedding = nn.Parameter(torch.empty(1, (image_size // patch_size)**2, hidden_dim).normal_(std=0.02))
        self.class_token = nn.Parameter(torch.empty(1, 1, hidden_dim))
        self.transformer_encoder = TransformerEncoder(num_layers, num_heads, hidden_dim, ffn_dim, dropout)
        self.proj = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        x = self.patch_embedding(x)
        x += self.pos_embedding
        batch_size = x.shape[0]
        batch_class_token = self.class_token.expand(batch_size, -1, -1)
        x = torch.cat([batch_class_token, x], dim=1)
        x = self.transformer_encoder(x)
        x = self.proj(x[:, 0, :])
        return x
```

---

```python
summary(VisionTransformer(),input_size=(16, 3, 224, 224))
```

will get:

```Bash
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
VisionTransformer                             [16, 1000]                151,296
├─PatchEmbedding: 1-1                         [16, 196, 768]            --
│    └─Conv2d: 2-1                            [16, 768, 14, 14]         590,592
├─TransformerEncoder: 1-2                     [16, 197, 768]            --
│    └─ModuleList: 2-2                        --                        --
│    │    └─TransformerEncoderLayer: 3-1      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-2      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-3      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-4      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-5      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-6      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-7      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-8      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-9      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-10     [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-11     [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-12     [16, 197, 768]            7,087,872
├─Linear: 1-3                                 [16, 1000]                769,000
```

---

```Bash
===============================================================================================
Total params: 86,565,352
Trainable params: 86,565,352
Non-trainable params: 0
Total mult-adds (G): 3.23
===============================================================================================
Input size (MB): 9.63
Forward/backward pass size (MB): 2575.69
Params size (MB): 345.66
Estimated Total Size (MB): 2930.98
===============================================================================================
```

---

And with `torch 2.1.0+cu118`:

```python
class VisionTransformer_torch(nn.Module):
    def __init__(
        self, image_size=224, in_channels=3, patch_size=16, num_classes=1000,
        num_layers=12, num_heads=12, hidden_dim=768, ffn_dim=3072, dropout=0.0
        ):
        super().__init__()
        self.patch_embedding = PatchEmbedding(in_channels, patch_size, hidden_dim)
        self.pos_embedding = nn.Parameter(torch.empty(1, (image_size // patch_size)**2, hidden_dim).normal_(std=0.02))
        self.class_token = nn.Parameter(torch.empty(1, 1, hidden_dim))
        transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dim_feedforward=ffn_dim, dropout=dropout, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(transformer_encoder_layer, num_layers=num_layers)
        self.proj = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        x = self.patch_embedding(x)
        x += self.pos_embedding
        batch_size = x.shape[0]
        batch_class_token = self.class_token.expand(batch_size, -1, -1)
        x = torch.cat([batch_class_token, x], dim=1)
        x = self.transformer_encoder(x)
        x = self.proj(x[:, 0, :])
        return x
```

---

```python
summary(VisionTransformer_torch(),input_size=(16, 3, 224, 224))
```

will, surprisingly, get the same total parameters (`86,565,352`), though the sizes (MB) are way smaller:

```Bash
===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
VisionTransformer_torch                       [16, 1000]                151,296
├─PatchEmbedding: 1-1                         [16, 196, 768]            --
│    └─Conv2d: 2-1                            [16, 768, 14, 14]         590,592
├─TransformerEncoder: 1-2                     [16, 197, 768]            --
│    └─ModuleList: 2-2                        --                        --
│    │    └─TransformerEncoderLayer: 3-1      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-2      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-3      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-4      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-5      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-6      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-7      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-8      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-9      [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-10     [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-11     [16, 197, 768]            7,087,872
│    │    └─TransformerEncoderLayer: 3-12     [16, 197, 768]            7,087,872
├─Linear: 1-3                                 [16, 1000]                769,000
```

---

```Bash
===============================================================================================
Total params: 86,565,352
Trainable params: 86,565,352
Non-trainable params: 0
Total mult-adds (G): 1.86
===============================================================================================
Input size (MB): 9.63
Forward/backward pass size (MB): 19.40
Params size (MB): 5.44
Estimated Total Size (MB): 34.47
===============================================================================================
```

---

#### §3.5.3 fine-tuning of ViT

[[2203.09795] _Three things everyone should know about Vision Transformers_](https://arxiv.org/abs/2203.09795):
- Parallel vision transformers.
- Fine-tuning attention is all you need.
- Patch preprocessing with masked self-supervised learning.


<!-- copy and paste till here -->

    </textarea>
    <script src="./layouts/remark-latest.min.js"></script>
    <script>
      var slideshow = remark.create({
        ratio: "16:9",
        highlightStyle: "github",
      });
    </script>
  </body>
</html>
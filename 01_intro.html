<!DOCTYPE html>
<html>
  <head>
    <title>§1 Intro</title>
    <meta charset="utf-8">

    <!-- KaTeX -->
    <link rel="stylesheet" href="./layouts/katex/katex.min.css">
    <script defer src="./layouts/katex/katex.min.js"></script>
    <script defer src="./layouts/katex/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "$", right: "$", display: false}
                ]
            });
        });
    </script>

    <!-- tabsets -->
    <link rel="stylesheet" href="./layouts/tabsets/tabsets.min.css">
    <script defer src="./layouts/tabsets/tabsets.min.js"></script>

    <link rel="stylesheet" href="./layouts/style.css" />
  </head>
  <body>
    <textarea id="source">

class: center, middle

<!-- copy and paste down below -->

## §1 Intro

OS, Conda, Tensor, `torch.nn`

---

### §1.1 OS

<center><img src="./assets/01_os.svg" width="3000"></center>

- [Anaconda](https://www.anaconda.com/), [Miniconda](https://docs.conda.io/en/latest/miniconda.html)
- [PyTorch](https://pytorch.org/)

---

### §1.2 Conda


- See the environments created:

    ```Bash
    conda info -e
    # or
    conda info --envs
    ```

- Create a new environment, such as `envName` (the name can be something you prefer):

    ```Bash
    conda create -n envName
    # remember to change the name!
    # or
    conda create --name envName
    # with python=3.7
    conda create -n envName python=3.7
    ```

- Activate a certain environment, such as `envName`：

    ```Bash
    conda activate envName
    ```

---

- Deactivate the current environment:

    ```Bash
    conda deactivate
    ```

- Delete a certain environment, such as `envName`:

    ```Bash
    conda remove -n envName --all
    ```

- Install packages, such as [numpy](https://github.com/numpy/numpy), [matplotlib](https://github.com/matplotlib/matplotlib), [torch](https://pytorch.org/), [torchinfo](https://github.com/TylerYep/torchinfo), [jupyterlab](https://github.com/jupyterlab/jupyterlab):

    ```Bash
    pip install numpy
    # or
    pip install numpy -i https://mirrors.aliyun.com/pypi/simple/
    # 临时从镜像阿里云下载
    ```

- See the packages installed in the `envName` environment:

    ```Bash
    conda activate envName
    conda list
    # or
    pip list
    ```

---


### §1.3 Tensor

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
```

---

#### §1.3.1 Shape

e.g. [H, W, C] (usually used in `numpy` or `matplotlib.pyplot`) or [C, H, W] (usually used in `torch`) or [batch_size, C, H, W].

<img src="https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-different-tensor-dimensions.png" width="320">

---

```python
dummy = torch.randn(1, 3, 32, 32)# [batch_size, C, H, W]
print(dummy.shape)
```

will get:

```Bash
torch.Size([1, 3, 32, 32])
```

Commonly used method to change the shape of a tensor:

- [`view()`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html)
- [`reshape()`](https://pytorch.org/docs/stable/generated/torch.reshape.html)
- [`unsqueeze()`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html)
- [`squeeze()`](https://pytorch.org/docs/stable/generated/torch.squeeze.html)

[`einops`](https://github.com/arogozhnikov/einops) provides a more intuive way to change the shape.

---

#### §1.3.2 Device

[torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch-device)

Tensor device:

```python
dummy = torch.randn(1, 3, 32, 32)
print(dummy.device)
dummy = dummy.to('cuda')
print(dummy.device)
dummy = dummy.to('cpu')
print(dummy.device)
```

will get:

```Bash
cpu
cuda:0
cpu
```

---

Model device:

```python
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        ...

    def forward(self, x):
        ...
        return x

model = Model()
model.to('cuda')
```

---

All tensors and objects (datasets, models) should be on the same device.

```python
dummy = torch.rand(1, 3, 32, 32).to('cuda')

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(32 * 32 * 32, 128)
        self.fc2 = nn.Linear(128, 10)
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = Model().to('cuda')
print(model(dummy).shape)
```

will get:

```Bash
torch.Size([1, 10])
```

---

#### §1.3.3 Type

[torch.dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype)

<div class="tabset"></div>

- `numpy.ndarray` -> `torch.Tensor`:
    
    float64 -> float32
    
    ```python
    dummy = np.random.rand(1, 3, 32, 32)
    print(dummy.dtype)
    dummy = torch.from_numpy(dummy)
    print(dummy.dtype)
    print(dummy.device)
    dummy = dummy.to(torch.float32)
    print(dummy.dtype)
    ```
    
    will get:
    
    ```Bash
    float64
    torch.float64
    cpu
    torch.float32
    ```

- `torch.Tensor` -> `numpy.ndarray`:
    
     `cuda` -> `cpu`, float32 -> float64
    
    ```python
    dummy = torch.rand(1, 3, 32, 32).to('cuda')
    print(dummy.dtype)
    print(dummy.device)
    dummy = dummy.to('cpu')
    dummy = dummy.numpy()
    print(dummy.dtype)
    dummy = dummy.astype('float64')
    print(dummy.dtype)
    ```
    
    will get:
    
    ```Bash
    torch.float32
    cuda:0
    float32
    float64
    ```

---

### §1.4 `torch.nn`

<div class="tabset"></div>

- nn.Conv2d
    
    [`nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)
    
    ```python
    x = torch.randn(1, 3, 28, 28)
    print(x.shape)
    
    conv2d = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3)
    x = conv2d(x)
    print(x.shape)
    ```
    
    will get:
    
    ```Bash
    torch.Size([1, 3, 28, 28])
    torch.Size([1, 12, 26, 26])
    ```

- nn.MaxPool2d
    
    [`nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)
    
    ```python
    x = torch.randn(1, 3, 28, 28)
    print(x.shape)
    
    pool = nn.MaxPool2d(kernel_size=2)
    x = pool(x)
    print(x.shape)
    ```
    
    will get:
    
    ```Bash
    torch.Size([1, 3, 28, 28])
    torch.Size([1, 3, 14, 14])
    ```

- nn.BatchNorm2d
    
    [`nn.BatchNorm2d`](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html), See Fig.2 of [[1803.08494] _Group Normalization_](https://arxiv.org/abs/1803.08494).
    
    ```python
    batchnorm = nn.BatchNorm2d(3)
    x = torch.randn(1, 3, 3, 3)
    print(x)
    
    x = batchnorm(x)
    print(x)
    ```

- nn.Linear
    
    [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). For fully connected layer.
    
    ```python
    linear = nn.Linear(3, 12)
    x = torch.randn(128, 3)
    x = linear(x)
    print(x.shape)
    ```
    
    will get:
    
    ```Bash
    torch.Size([128, 12])
    ```

- nn.Dropout
    
    [`nn.Dropout`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html). For fully connected layer. Using the samples in the Bernoulli distribution, some elements of the input tensor are randomly zeroed with probability $p$. To use it:
    
    ```python
    dropout = nn.Dropout(p=0.5, inplace=False)
    x = dropout(x)
    ```
    
    `x` can be a tensor in any shape.

- nn.ReLU or F.relu
    
    [`nn.ReLU`](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html), [`F.relu`](https://pytorch.org/docs/stable/generated/torch.nn.functional.relu.html). Activation function, $\text{ReLU}(x)=\max{(0,x)}$, to use it：
    
    ```python
    x = nn.ReLU(x)
    ```
    
    or:
    
    ```python
    x = F.relu(x)
    ```
    
    `x` can be a tensor in any shape.

- nn.RNN
    
    [`nn.RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)
    
    ```python
    input_size = 10
    hidden_size = 20
    num_layers = 2
    seq_length = 5
    batch_size = 3
    rnn = nn.RNN(input_size, hidden_size, num_layers)
    input_data = torch.randn(seq_length, batch_size, input_size)
    output, hidden_state = rnn(input_data)
    print(output.shape)
    ```
    
    will get:
    
    ```Bash
    torch.Size([5, 3, 20])
    ```

- nn.Module
    
    [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). Construct a block of layers. It could be the entire model or just a block of the entire model or loss function, etc.
    
    ```python
    class MyBlock(nn.Module):
        def __init__(self):
            super().__init__()
            # define every layer
            self.conv1 = nn.Conv2d(1, 20, 5)
            self.conv2 = nn.Conv2d(20, 20, 5)
    
        def forward(self, x):
            # define forward propagation
            x = F.relu(self.conv1(x))
            x = F.relu(self.conv2(x))
            return x
    ```

- nn.Sequential
    
    [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). Compared with `nn.Module`, `nn.Sequential` can add the layers more easily and don't have to define forward propagation. This is more useful when building a simple neural network.
    
    ```python
    model = nn.Sequential(
        nn.Conv2d(1, 20, 5),
        nn.ReLU(),
        nn.Conv2d(20, 64, 5),
    )
    
    x = torch.randn(1, 1, 30, 30)
    y = model(x)
    print(y.shape)
    ```
    
    will get:
    
    ```Bash
    torch.Size([1, 64, 22, 22])
    ```


<!-- copy and paste till here -->

    </textarea>
    <script src="./layouts/remark-latest.min.js"></script>
    <script>
      var slideshow = remark.create({
        ratio: "16:9",
        highlightStyle: "github",
      });
    </script>
  </body>
</html>